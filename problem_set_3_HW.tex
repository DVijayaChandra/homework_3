\documentclass[11pt]{article}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.0in}
\setlength{\topmargin}{-.1875pt}
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\headsep}{0pt}
\setlength{\parskip}{1ex}
\setlength{\headheight}{0pt}
\usepackage{graphicx}
%\usepackage{caption}
\usepackage{amsmath}
\begin{document}
\def\Q#1#2{\vspace{1ex}\hrule\vspace{1ex}{\bf \noindent Question #1: #2}\\}
\def\text#1{{\rm #1}}
%----------------------------------------------------------------------
\begin{center}
\section*{Homework Problem Set 3: Basic Clustering}
\end{center}
Due Thursday, Jan. 26 at 11:59 PM  \\
Upload a pdf to Canvas  \\ \\
Each question is worth the same number of points. \\ \\
%----------------------------------------------------------------------
\Q{1}{}
1.	For the following questions, give an answer (True/False) and a short (1 or 2 sentences) explanation.   For the rest of this question, “agglomerative hierarchical clustering” refers to procedures such as single link, complete link, and group average, while “k-means clustering” refers to k-means with random initialization of centroids and Euclidean distance.
\begin{itemize}
  \item[a)]	Agglomerative hierarchical clustering procedures are better able to handle outliers than k-means.
% Answ: 
%True. When using agglomerative clustering, outliers will tend to remain as singleton clusters (or small clusters) that do not merge with other clusters until high in the tree. This makes it easy for agglomerative clustering techniques to identify and eliminate outliers. The standard K-means clustering algorithm, by contrast, assigns each outlier to some cluster, and thus can distort the centroid of the cluster.

  \item[b)]	For any given data set, different runs of k-means can produce different clusterings, but agglomerative hierarchical clustering procedures will always produce the same clustering.
% Answ: 
% True. There is no random element in the algorithms for agglomerative hierarchical techniques unless there are ties in the proximity values.

  \item[c)]	K-means take less time and memory than agglomerative hierarchical clustering and is the most efficient clustering algorithm possible.
% Answ: 
%False.  Although k-means is more computationally efficient than agglomerative hierarchical clustering, there are more efficient algorithms possible, e.g., the leader algorithm. (See Exercise 12, Chapter 7.)

  \item[d)]	During a post-processing step for K-means, a cluster is split by picking one of the points of the cluster as a new centroid and then reassigning the points in the cluster either to the original centroid or the new centroid. What happens to the SSE of the clustering?
% Answ: 
%Splitting decreases SSE because we have two centroids for the same set of points, which will reduce the distance of points to the nearest centroid.

  \item[e)]	When clustering a dataset using K-means, whenever SSE decreases, cohesion increases.
% Answ: 
%True. For K-means, SSE is an inverse measure of the cohesion of clusters, and thus, as SSE decreases, cohesion increases and vice-versa.

  \item[f)]	When clustering a dataset using K-means, whenever SSB (the between sum of squares) increases, separation increases.
% Answ: 
%True. For K-means SSB (the between sum of squares) is a direct measure of the separation of clusters, and thus, as SSB increases, separation increases and vice-versa.

  \item[g)]	Cohesion and separation are independent for K-Means, i.e., improving cohesion (smaller SSE) doesn’t necessarily improve separation (larger between sum of squares (SSB)).
% Answ: 
%False. SSE and SSB are related because the total sum of squares (TSS) is a constant and TSS = SSE + SSB. So, improving cohesion (reducing SSE) will also improve separation (increasing SSB, and vice-versa. See the book, page 577.

  \item[h)]	When clustering a dataset using K-means, SSE + BSS is a constant.
% Answ: 
%True. For K-means, the total sum of squares (TSS) is the sum of SSE (or within cluster sum of squares) and the between sum of squares (SSB), TSS is constant during the K-means clustering process. See the book, page 577

  \item[i)]	When clustering a dataset using K-means, whenever cohesion increases, separation increases.
% Answ: 
%True. For K-means, the total sum of squares (TSS) is the sum of SSE (the within cluster sum of squares) and the between sum of squares (SSB). Note TSS is constant at every step of the K-means clustering process. See the book, page 577.  SSE is an inverse measure of cluster cohesion, while SSB is a direct measure of cluster separation. Thus, as cohesion increases, SSE decreases, and SSB (separation) increases since TSS = SSE + SSB is a constant. When SSE is at a local minima, BSS is at a local maxima.

%j)	K-means only optimizes cluster cohesion (SSE). Hence, improving cohesion (smaller SSE) doesn’t necessarily improve cluster separation.
%% Answ: 
%%False. For K-means, the total sum of squares (TSS) is the sum of SSE (the within cluster sum of squares) and the between sum of squares (SSB). Note TSS is constant at every step of the K-means clustering process. See the book, page 577.  SSE is an inverse measure of cluster cohesion, while SSB is a direct measure of cluster separation. Thus, as cohesion increases, SSE decreases, and SSB (separation) increases since TSS = SSE + SSB is a constant.
%
%k)	When clustering a dataset using K-means, SSE is guaranteed to monotonically decrease as the number of clusters increases.
%% Answ: 
%%False. SSE will generally decrease as k, the number of clusters, increases, but because of random initialization and local minima, it can happen that the SSE for a particular run of K-means with a particular k will be lower than for some other run of K-means with a smaller value of k.
%
%l)	Hierarchical clustering approaches can only handle low-dimensional data.
%% Answ: 
%%False. Hierarchical clustering algorithms use a proximity matrix. Thus, if a given similarity or
%%dissimilarity is appropriate for high dimensions, then they can cluster high dimensional data.
%
%m)	Agglomerative hierarchical clustering doesn’t optimize any global objective function.
%% Answ: 
%%True. The algorithms operate in a greedy manner.
%
%n)	For density-based clusters, it is difficult to define a natural measurement of cohesion.
%% Answ: 
%%True, since the natural way to define a measure of cohesion would have a low value if all the points of a cluster are relatively farther from points in other clusters than from points in their cluster, but this is often not the case for density-based clusters.
%
%o)	The silhouette coefficient is a good evaluation measure for density-based clusters.
%% Answ: 
%%False. The silhouette coefficient combines measures of cohesion and separation, but these measures may not work well for density-based clustering. For example, the density-based clusters found by DBSCAN can have widely different densities, i.e., different cohesions and may be intertwined, i.e., poor separation.
%
%p)	Gaussian mixture model clustering cannot handle varying densities.
%% Answ: 
%%False. GMM clustering can even handle embedded clusters (where one cluster sits inside another), where one cluster is denser than another.
%
%q)	The clusters for Gaussian mixture model clustering are fully described by their cluster centroids.
%% Answ: 
%%False. A full description of GMM clusters also needs to include the covariance matrix of each cluster.
%
%r)	Like SNN and DBSCAN, Chameleon is a partitional clustering algorithm.
%% Answ: 
%%False. Chameleon is a hierarchical clustering scheme.
%%Note: A hierarchical clustering does produce a partitional clustering at any given level. However, SNN and DBSCAN produce only a single partitional clustering, and should not be confused with a hierarchical clustering, which is a sequence of nested partitional clusterings.
%
%s)	Shared Nearest-Neighbor Clustering is a complete clustering algorithm
%% Answ: 
%%False. SNN clustering discards the noise points.
%
%t)	On high dimensional data, DBSCAN is likely to work better than Shared Nearest-Neighbor Clustering.
% Answ: 
%False, density is not defined well in higher dimensions.
%----------------------------------------------------------------------
%2.	To answer the following true/false question about how k-means operates, refer to figure (a) below.  Note that we are referring to the very basic k-means algorithm presented in class and not to any of it more sophisticated variants, such as bisecting k-means or k-means++.
%Note that for the figure, the initial centroids are given by the symbol:
%For figure (a), assume the shaded areas represent points with the same uniform density.
%True or False: For Figure (a) and the given initial centroid: When the k-means algorithm completes, each shaded circle will have one cluster centroid at its center.

% ANSW
% True. The clusters are too far away for one centroid to attract points from another.
%----------------------------------------------------------------------
\newpage
\Q{2}{}
To answer the following true/false questions about how k-means operates, refer to figures (a), (b), and (c), below.  Note that we are referring to the very basic k-means algorithm presented in class and not to any of it more sophisticated variants, such as bisecting k-means or k-means++.
Note that for all three figures, the initial centroids are given by the symbol:
For figures (a) and (b), assume the shaded areas represent points with the same uniform density. For Figure (c), the data points are given as red dots, and their values are indicated under the dots. No explanation for your answer is necessary unless you feel there is some ambiguity in the figure or the question.
\includegraphics[scale=.5]{2023-02-09_08-12-49_3questions.png}

\begin{itemize}
\item[a)]	True or False: For Figure (a) and the given initial centroid: When the k-means algorithm completes, each shaded circle will have one cluster centroid at its center.
% Answer: True. The clusters are too far away for one centroid to attract points from another.
\item[b)]	True or False: For Figure (b) and the given initial centroids: When the k-means algorithms completes, there will be one cluster centroid in the center of each of the two shaded regions, and each of the two final clusters will consist only of points from one of the shaded regions. In other words, none of the two final clusters will have points from both shaded regions.
% Answer: False.  The final clusters will have points from both of the two shaded regions since they are close to each other and not of circular shape.
\item[c)]True or False:   For Figure (c) and the given initial centroids, the final clustering for k-means contains an empty cluster.
% Answer. True.  The centroid at 12.5 is farther away from all points than any other clusters and will become empty.
\end{itemize}
%----------------------------------------------------------------------
\Q{3}{}
Consider the four data points shown in the following Figure. The distance between each data point to the center C is $R$.
\begin{center}
\includegraphics[scale=.5]{2023-02-09_08-23-03_SSE.png}
\end{center}
\begin{enumerate}
  \item[a)]	Compute the total SSE of the data points to the centroid, C.
%Answ:  4R^2
  \item[b)]	Compute the total SSE of the data points to the origin, O.
%Answ:  4(a^2 + b^2 + R^2)
  \item[c)]	Using parts (a) and (b), compute the SSE for the 8 data points shown below with respect to the centroid, D. Note that points u, v, w, and x lie on a circle of radius $R/2$. Also, the figure is symmetric with respect to the horizontal line running through D.
%Answ: using part (a), the SSE of u, v, w, and x with respect to their midpoint is 4(R/2)2. 
%From part (b), with a=R and b=0, the SSE of u, v, w, and x with respect to D is 
%4(R2 + 02 + (R/2)2)  = 5R2.
%By symmetry, the SSE of the eight points with respect to D is 10*R^2.
\end{enumerate}
\begin{center}
\includegraphics[scale=.5]{2023-02-09_08-26-34_SSE_symmetry.png}
\end{center}
%----------------------------------------------------------------------
\Q{4}{} % Question 11 from cluster .docx
In each of the three sets of figures below, assume that circles A and B contain 100 points each, and circle C contains 100,000 points. The X’s are the centroid initializations for each run of K-means clustering. Assume a uniform distribution of points within each circle. Each circle is the same size, and the distances between the circles is to scale.

For each figure, you should tell how many centroids should end up in each circle after convergence of K-means clustering. Your answer should be 0, 1, 2, or 3. You should provide a brief justification for each case.
%\begin{figure}
\begin{center}
\includegraphics[scale=.5]{2023-02-09_08-44-40_case_a.png}
  \begin{center} Figure 3 (a) \end{center}
\end{center}

\begin{center}
\includegraphics[scale=.5]{2023-02-09_08-44-50_case_b.png}
  \begin{center} Figure 3 (b) \end{center}
\end{center}

\begin{center}
\includegraphics[scale=.5]{2023-02-09_08-45-03_case_a.png}
  \begin{center} Figure 3 (c) \end{center}
\end{center}

%\end{figure}
\begin{enumerate}
  \item[a)]  The distance between circles A and B is the same as the distance between B and C. (Figure 3 (a))
%Answer
%Number of Centroids in Circle (a): 1
%Number of Centroids in Circle (b): 1
%Number of Centroids in Circle (c): 1
%Brief explanation: All of circle A’s points will be assigned to the centroid in A. About ⅓ of circle B’s points (the ones in the left third of circle B) will be assigned to the centroid on the left in circle B. The remaining ⅔ of the points in B and all the points in C will be assigned to the centroid in the center of B. This will cause the right centroid in B to move to circle C since C has many more points than B.  In the next iteration, all points in A,B, and C will be assigned to the centroids located in their own circles and K-means will converge.
  \item[b)] The distance between circles A and B is the same as the distance between B and C. (Figure 3 (b))
%\begin{figure}
%\caption{GORDON}
%\end{figure}
% Answer
%Number of Centroids in Circle (a): 1
%Number of Centroids in Circle (b): 1
%Number of Centroids in Circle (c):  1
%Brief explanation: Since circles A and B are close together and quite far away from circle C, the points from both A and B will be assigned to the centroid that is in A. The points in C will be split between the two centroids in C, with each centroid having 50,000 points.
%Since A and B have the same number of points each, the centroid in A will move between A and B. The centroids in C will move apart slightly but both will remain in C, each having half of C’s points.
  \item[c)] Circles A and B are much closer than B and C. (Figure 3 (c))
%\caption{GORDON}
% Answer
%Number of Centroids in Circle (a): 0
%Number of Centroids in Circle (b):  0
%Number of Centroids in Circle (c):  2
%Brief explanation: 
%Since circles A and B are close together and quite far away from circle C, the points from both A and B will be assigned to the centroid that is in A. The points in C will be split between the two centroids in C, with each centroid having 50,000 points. 
%Since A and B have the same number of points each, the centroid in A will move between A and B. The centroids in C will move apart slightly but both will remain in C, each having half of C’s points.
\end{enumerate}
% WRONG: answer (b) and (c) are the same. NOT CORRECT
%----------------------------------------------------------------------
\Q{5}{}
At an intermediate stage of some agglomerative clustering algorithm, you are given three groups of points, as shown in the figure below, which need to be considered for merging. Note that every circle in the figure represents a two-dimensional point, and the Euclidean distance in two dimensions is being used as the distance measure.
\begin{center}
\includegraphics[scale=.5]{2023-02-09_09-17-33_hierarch1.png}
  \begin{center} Figure 4 \end{center}
\end{center}
\begin{enumerate}
  \item[a)]	Using the single link (MIN) hierarchical clustering technique, which pair of groups would you consider for merging? Provide a one-sentence justification.
%ANSW:
% Groups A and B will be merged since they have the smallest single link distance (between the right-most point of A and left-most point of B), as compared to Groups A and C, and Groups B and C.
  \item[b)]	Using the complete link (MAX) hierarchical clustering technique, which pair of groups would you consider for merging? Provide a one-sentence justification.
%ANSW:
% Groups A and C will be merged since they have the smallest complete link distance (between the right-most point of A and the farthest point in C), as compared to the complete link distance of Groups A and B (between the left-most point in A and right-most point in B), and Groups B and C (between right-most-point in B and the farthest point in C).
\end{enumerate}
%----------------------------------------------------------------------
\Q{6}{}
Suppose we apply DBSCAN to cluster the following dataset using Euclidean distance.
\begin{center}
  \includegraphics[scale=.4]{2023-02-09_09-23-27_dbscan.png}
\begin{center} Figure 5 \end{center}
\end{center}
A point is a core point if its density (number of points within $\epsilon$) is $\geq$ MinPts. 
Given that MinPts = 3 and
EPS = 1, answer the following questions.
\begin{enumerate}
  \item[a)] Label all points as “core points,” “boundary points,” and “noise.”
%A core point, as defined here, has its distance to its two nearest neighbors  1.
%Core points: B, C, E, F, I, J, L, M
%Border points: D, G
%Noise: A, H
	\item[b)] What is the clustering result?
% Answ:
%There will be two clusters. B, C, D, E, F, G will form a cluster, and I, J, L, M, will form the other cluster.
	\item[c)] Repeat the above two questions when $\epsilon = \sqrt{2}$.
% ANSW:
%Now the core points are defined as points with distance to its 2-NN <= 2.
%Core points: B, C, E, F, I, J, L, M, D, G,
%Border points: A, H
%All points are clustered as a single cluster.
\end{enumerate}
%----------------------------------------------------------------------
\Q{7}{}
The following table (confusion matrix) shows the k-means clustering results for a land cover classification dataset that consists of many pieces of land. The number provided in the table is the number of objects (pieces of land) that are clustered into each cluster that belongs to each category. For example, the number in the forest column and cluster 1 row means that 10 forest items are clustered into cluster 1. Answer the following questions based on the given table. No calculations are necessary. Briefly explain your answer. 

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|} \hline
            &	Forest	&Farm &	Shrubland &	Urban &	Water \\
  Cluster 1 &	10 &	100	 &20	 &10	& 30000 \\
  Cluster 2	& 3000	& 10	& 1000	& 10	& 0 \\
  Cluster 3	& 10	& 3000	& 500	& 150	& 200 \\
  Cluster 4	 &2000	& 2500 &	1500	& 3000	& 1400  \\ \hline
\end{tabular} 
  \begin{center}
Table: k-means clustering results for land cover classification dataset
  \end{center}
\end{center}
\begin{enumerate}
\item[a)]	Which cluster has the largest clustering entropy?  
% Answer: Cluster 4. Reason: most random. 
\item[b)]	Which cluster has the smallest clustering entropy?  
% Answer: Cluster 1. Reason: least random. 
\end{enumerate}
%----------------------------------------------------------------------
\newpage
\Q{8}{}
The figures below are sorted according to cluster labels, and corresponds to the sets of points (Dataset X, Dataset Y, and Dataset Z). Differences in color distinguish between clusters, and each set of points contains 100 points and four clusters each of equal size. In the distance matrix, blue indicates the lowest distances and red indicates the highest distances.

\begin{center}
  \includegraphics[scale=.4]{2023-02-09_09-58-34_similarity.png}
\begin{center} Figure 5 \end{center}
\end{center}

\begin{enumerate}
  \item[(a)]	Match the distance matrices (Matrix 1, Matrix 2, Matrix 3) with the sets of points (Dataset X, Dataset Y, and Dataset Z). Provide a brief explanation of the diagaonal entries and the non-diagonal entries. 
% Answer
%Diagonal entries explanation:
%-	Diagonal entries are all similarly crisp and blus, the most crisp compared to other confusion matrices, indicating that points that belong to the same cluster are closest to each other, and cluster cohesion is similar for all clusters
%Off-diagonal entries explanation:
%1.	Rows 1 and 3 correspond to clusters A and C. This is because the colors of the off- diagonal entries for these two rows are all different, indicating the different distances between cluster A (or C)’ s distances to all other clusters (i.e: A is closest to C (blue off-diagonal); followed by B (green off-diagonal); and is the furthest from D (yellow off-diagonal); similar explanation for C).
%2.	Row 2 correspond to cluster B. Same distances to A and C (green off-diagonal), furthest distance from A (red off-diagonal)
%3.	Row 4 correspond to cluster D. Same distances to A and C (yellow off-diagonal), furthest distance from B (red off-diagonal).
%
%  ANSWER IS PARTLY MISSING
%ii.	Matrix 2
%Dataset (X/Y/Z):  X
%Explanation
%Diagonal entries explanation:
%-	2 diagonal entries are more blue and crisp compared to the other 2, indicating 2 clusters have better cohesion (B and C) than the other 2 (A and D)
%Off-diagonal entries explanation:
%1.	  Rows with less crisp diagonal entries (rows 1 and 4) have all different colors, indicating that all other clusters have different distances from these clusters (e.g: Cluster A is the nearest to B, followed by C and then D, no 2 clusters have same distance to cluster A)
%2.	    Rows with more crisp diagonal entries have 2 same colors (other than the diagonal), indicating that it has same distance to 2 clusters, and is the furthest from 1 cluster (e.g: B’s distance to A and C is similar, but is the furtherst from D)
%iii.	Matrix 3
%Dataset (X/Y/Z): Y
%Explanation:
%Diagonal entries explanation:
%-   2 diagonal entries are more blue and crisp compared to the other 2, indicating 2 clusters have better cohesion (B and C) than the other 2 (A and D)
%Off-diagonal entries explanation:
%1.	All rows have 2 similar and 1 different colors off diagonals entries. This indicates each cluster has 2 other clusters relatively closer to it than the remaining 1 cluster (e.g: B is similarly close to A and C compared to with D)
  \item[(b)] For the symmetric matrix given in Matrix 2 match the four rows to the corresponding clusters (characterized the nearest alphabet in 
  each cluster (e.g: A, B, C, D)) in the dataset that you match with it in the previous question. Provide a brief explanation
% Answer
%  i.	Matrix rows: 1st (1-100) Cluster (A/B/C/D): A Explanation:
%Diagonal entry is less crisp, meaning the cluster is less cohesive. All off- diagonal entries have different colors, indicating all other clusters have different distances from is (closest to B, fllowed by C, and furthest from A)
%
%ii.	Matrix rows: 2nd (101-200) Cluster (A/B/C/D): B Explanation:
%
%Diagonal entry is more crisp, indicating the cluster is cohesive. 2/3 off-diagonal entries have the same color, indicating 2 other clusters are closer to it (A and C, eventhough the off-diagonal indicating distances with A is less crisp), and is the furthest from 1 other cluster (D)
%
%iii.	Matrix rows: 3rd (201-300) Cluster (A/B/C/D): C Explanation: Similar to (ii)
%
%
%iv.	Matrix rows: 4th (301-400)
%Cluster (A/B/C/D): D
%Explanation: Similar to (i) in inverted order
\end{enumerate}
%----------------------------------------------------------------------
\Q{9}{}
For each of the described data sets, decide what type of clustering should be used (hierarchical or partitional, exclusive or overlapping or fuzzy, complete or partial (incomplete)). Briefly explain your reasoning if you feel there may be several possible answers. Note: we are using partitional and hierarchical in the more relaxed use of the terms to mean un-nested or nested, respectively.

An example: Clustering library books based on their literary genre. The genre/topic can have several subtopics, as well.
Answer: hierarchical, overlapping, complete
\begin{itemize}
  \item[a)]	Proteins perform different biological functions that are organized into a hierarchical taxonomy (GO) defined by biologists. Some proteins can be multi-functional as well. You want to group them based on those functions. Some proteins may also be missing functional annotation.
% Answ: Hierarchical, overlapping, partial
\item[b)]	A nutritionist asks you several questions (e.g., your calorie intake, types of food you eat, your physical activity labels, and so on) to assess your risks for diabetes in three different groups: low, medium, and high.
% Answ: Partitional, exclusive, complete
\item[c)]
An international grad student can work on campus only at most for 20 hours. You want to assign each student to different job categories (e.g., TA, RA, another on-campus job, jobless). Hint: the sum of these categories should sum up to 20 hours.
% Answ: Partitional, fuzzy, complete
\item[d)]	Grouping of students in a university-based on the organization (department, college, institute, etc.)  to which they belong. A student may belong to multiple organizations. Also, some students don’t have declared majors and hence may not belong to any organization.
 % Answ: Hierarchical, overlapping, partial.
\item[e)]	Grouping of all the students in the Computer Science department based on the letter grade they get in the data mining (CSci 5523) class.
% Answ: Partitional, exclusive, partial (some students in the CS dept wouldn’t have taken the DM class and thus can’t be grouped).
\end{itemize}
%----------------------------------------------------------------------
\Q{10}{}
Consider the following two sets of points (faces) shown in figures (a) and (b). The darker shading indicates a denser point distribution.
\begin{center}
\includegraphics[scale=.4]{2023-02-09_10-32-12_smiley.png}
\end{center}
\begin{itemize}
\item[a)]	For each figure, could you use DBSCAN to find clusters corresponding to the patterns represented by the nose, eyes, and mouth? Explain.
% Answ: 
%	DBSCAN can work only for (b) because in (b) the points in the nose, eyes, and mouth are much closer together than the points between these areas, and DBSCAN could distinguish these areas. For (a), the noise is much denser than the interest patterns, so the nose, eyes, and mouth will be eliminated by DBSCAN.
\item[b)]	For each figure, could you use K-means to find the patterns represented by the nose, eyes, and mouth? Explain.
%Answ: 
% K-means can work for (b) as long as the number of clusters was set to 4, although the lower density points would also be included. K-means does not work for (a).
\item[c)]	For (a), could you figure out a clustering method, which can find the patterns represented by the nose, eyes, and mouth?
%Answ: Take the reciprocal of the density as the new density and use DBSCAN.

\end{itemize}
%----------------------------------------------------------------------
\end{document}
